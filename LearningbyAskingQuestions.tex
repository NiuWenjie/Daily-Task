\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{indentfirst}
\usepackage{epsfig}
\usepackage{picinpar,graphicx}
\usepackage[breaklinks=true,bookmarks=false,colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green,
            backref=page]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\begin{document}

%%%%%%%%% TITLE
\title{Learning by Asking Questions}

\author{Wenjie Niu\\\\ June 8,2018}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  \cite{Misra_2018_CVPR}We introduce an interactive learning framework for the
development and testing of intelligent visual systems, called
learning-by-asking (LBA). We explore LBA in context of the
Visual Question Answering (VQA) task. LBA differs from
standard VQA training in that most questions are not observed
during training time, and the learner must ask questions
it wants answers to. Thus, LBA more closely mimics
natural learning and has the potential to be more dataefficient
than the traditional VQA setting. We present a
model that performs LBA on the CLEVR dataset, and show
that it automatically discovers an easy-to-hard curriculum
when learning interactively from an oracle. Our LBA generated
data consistently matches or outperforms the CLEVR
train data and is more sample efficient. We also show that
our model asks questions that generalize to state-of-the-art
VQA models and to novel test time distributions.\par
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Machine learning models have led to remarkable
progress in visual recognition. However, while the training
data that is fed into these models is crucially important,
it is typically treated as predetermined, static information.
Our current models are passive in nature: they rely on training
data curated by humans and have no control over this
supervision. This is in stark contrast to the way we humans
learn — by interacting with our environment to gain information.
The interactive nature of human learning makes
it sample efficient (there is less redundancy during training)
and also yields a learning curriculum (we ask for more complex
knowledge as we learn).\par
In this paper, we argue that next-generation recognition
systems need to have agency — the ability to decide what
information they need and how to get it. We explore this in
the context of visual question answering (VQA; \cite{Stanislaw_2015_CVPR},\cite{Justin_2017_CVPR},\cite{Yuke_2016_CVPR}).
Instead of training on a fixed, large-scale dataset, we propose
an alternative interactive VQA setup called learningby-asking
(LBA): at training time, the learner receives only images and decides what questions to ask. Questions asked
by the learner are answered by an oracle (human supervision).
At test-time, LBA is evaluated exactly like VQA using
well understood metrics.

\begin{figure}[!htb]
\begin{center}
   \includegraphics[width=1\linewidth]{InvalidQuestions.png}
\end{center}
   \caption{Examples of \textbf{invalid} questions for images in the
CLEVR universe. Even syntactically correct questions can
be invalid for a variety of reasons such as referring to absent
objects, incorrect object properties, invalid relationships in
the scene or being ambiguous, \emph{etc}.}
\label{fig:questions}
\end{figure}

\textbf{Active learning}(AL) involves a collection of unlabeled
examples and a learner that selects which samples will be
labeled by an oracle~\cite{Kapoor2007Active},~\cite{Li2013Adaptive},~\cite{settles2009active},~\cite{Vijayanarasimhan2014Large}. Common selection
criteria include entropy~\cite{Joshi2009Multi}, boosting the margin for classi-
fiers~\cite{Collins2008Towards} and expected informativeness~\cite{Neil2011Bayesian}. Our setting
is different from traditional AL settings in multiple ways.
First, unlike AL where an agent selects the image to be labeled,
in LBA the agent selects an image and generates a
question. Second, instead of asking for a single image level
label, our setting allows for richer questions about objects,
relationships etc. for a single image. While~\cite{Choi2016Knowledge},~\cite{Siddiquie2010Beyond} did
use simple predefined template questions for AL, templates
offer limited expressiveness and a rigid query structure. In
our approach, questions are generated by a learned language
model. Expressive language models, like those used in our
work, are likely necessary for generalizing to real-world
settings. However, they also introduce a new challenge:
there are many ways to generate invalid questions, which
the learner must learn to discard (see Figure~\ref{fig:questions}).

%-------------------------------------------------------------------------

{\small
\bibliographystyle{ieee}
\bibliography{LearningbyAskingQuestions}
}

\end{document}